{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Framework - Computer Vision / CNN\n",
    "\n",
    "In this tutorial, we will show you how to prepare a dataset for a convolutional neural network. We will do the following:\n",
    "\n",
    "1. Preprocess a collection of images of fruits from the Kaggle Fruits-360 dataset into Machine Learning ready data.\n",
    "2. Store the Machine Learning ready data into a repository.\n",
    "3. Create a batch feeder.\n",
    "4. Create a CNN.\n",
    "5. Retreive the Machine Learning ready data.\n",
    "6. Train the CNN with our Machine Learning ready data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\'\\Desktop\\Gap-ml\n"
     ]
    }
   ],
   "source": [
    "# Let's go the directory of the Gap Framework\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Let's start by importing the Gap <b style='color:saddlebrown'>vision</b> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Gap Vision module\n",
    "from vision import Image, Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to a respository of images for classifying types of fruits. We will use this repository for image preprocessing for computer vision.\n",
    "\n",
    "The training and test datasets are under the corresponding subfolders Training and Test. Each subfolder under Training (and Test) is named according to the type of fruit (e.g., Apple) and optionally followed by a variety (e.g., Red Delicious).\n",
    "\n",
    "Let's take a look at the subfolders and see how many different classes of fruits are in our training set (i.e., 76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels: 77\n",
      "['Apple Braeburn', 'Apple Golden 1', 'Apple Golden 2', 'Apple Golden 3', 'Apple Granny Smith', 'Apple Red 1', 'Apple Red 2', 'Apple Red 3', 'Apple Red Delicious', 'Apple Red Yellow', 'Apricot', 'Avocado', 'Avocado ripe', 'Banana', 'Banana Red', 'Cactus fruit', 'Cantaloupe 1', 'Cantaloupe 2', 'Carambula', 'Cherry 1', 'Cherry 2', 'Cherry Rainier', 'Cherry Wax Black', 'Cherry Wax Red', 'Cherry Wax Yellow', 'Clementine', 'Cocos', 'Dates', 'fruits.h5', 'fruits360.zip', 'Granadilla', 'Grape Pink', 'Grape White', 'Grape White 2', 'Grapefruit Pink', 'Grapefruit White', 'Guava', 'Huckleberry', 'Kaki', 'Kiwi', 'Kumquats', 'Lemon', 'Lemon Meyer', 'Limes', 'Lychee', 'Mandarine', 'Mango', 'Maracuja', 'Melon Piel de Sapo', 'Mulberry', 'Nectarine', 'Orange', 'Papaya', 'Passion Fruit', 'Peach', 'Peach Flat', 'Pear', 'Pear Abate', 'Pear Monster', 'Pear Williams', 'Pepino', 'Physalis', 'Physalis with Husk', 'Pineapple', 'Pineapple Mini', 'Pitahaya Red', 'Plum', 'Pomegranate', 'Quince', 'Rambutan', 'Raspberry', 'Salak', 'Strawberry', 'Strawberry Wedge', 'Tamarillo', 'Tangelo', 'Walnut']\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../FruitMaps/fruits/fruits-360/Training\")\n",
    "\n",
    "# Let's get a list of all the subfolders of collections of fruits\n",
    "labels = os.listdir()\n",
    "print(\"Number of Labels:\", len(labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look a little closer at the images in the training set. We will dive into the first subfolder (Apple Braeburn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 492\n"
     ]
    }
   ],
   "source": [
    "# Let's get a listing of all the images in the first subfolder.\n",
    "data = os.listdir(labels[0])\n",
    "print(\"Number of Images:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use openCV to get some basic information on the images. From the shape of the pixel data we see that its a 100x100 pixel image with three channels (i.e., RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_100.jpg\n",
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Import the openCV module\n",
    "import cv2\n",
    "\n",
    "# We will look at the first image in this first collection.\n",
    "print(data[0])\n",
    "\n",
    "# Use openCV to read the image into memory as an uncompressed bitmap\n",
    "pixels = cv2.imread(labels[0] + '/' + data[0])\n",
    "\n",
    "# Let's look at the shape of the image.\n",
    "print(pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few more random images in this subfolder and see if they are all the same size and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106_100.jpg\n",
      "(100, 100, 3)\n",
      "123_100.jpg\n",
      "(100, 100, 3)\n",
      "229_100.jpg\n",
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Our random selection of images\n",
    "for index in [ 7, 26, 143 ]:\n",
    "    print(data[index])\n",
    "\n",
    "    # Use openCV to read the image into memory as an uncompressed bitmap\n",
    "    pixels = cv2.imread(labels[0] + '/' + data[index])\n",
    "\n",
    "    # Let's look at the shape of the image.\n",
    "    print(pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the are the same size.\n",
    "\n",
    "Let's look a different collection of fruits and see if they too are the same size. Let's use the 8th (index 7) subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106_100.jpg\n",
      "(100, 100, 3)\n",
      "123_100.jpg\n",
      "(100, 100, 3)\n",
      "229_100.jpg\n",
      "(100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Our random selection of images\n",
    "for index in [ 7, 26, 143 ]:\n",
    "    print(data[index])\n",
    "\n",
    "    # Use openCV to read the image into memory as an uncompressed bitmap\n",
    "    pixels = cv2.imread(labels[7] + '/' + data[index])\n",
    "\n",
    "    # Let's look at the shape of the image.\n",
    "    print(pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Let's do a practice run and preprocess one collection of fruit images.\n",
    "\n",
    "Note, how we specified the subfolder instead of a list for the parameter images. The initializer (constructor) looks at the parameter and if its not a list, but a string it presumes the parameter is a path to a folder with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 8.034014225006104\n"
     ]
    }
   ],
   "source": [
    "images = Images(labels[0], 0)\n",
    "print(\"TIME\", images.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps our images won't need to be as big to train the CNN. Let's take a shot in the dark and say they only need to be 50x50. This will reduce the size of our data by 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 7.987214088439941\n"
     ]
    }
   ],
   "source": [
    "images = Images(labels[0], 0, config=['resize=(50,50)'])\n",
    "print(\"TIME\", images.time)\n",
    "\n",
    "os.remove('collection.0_100.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "The labels of the fruits are names, but we need integer values to train the CNN. Since all the subfolder names (fruit name+variety) are in the list labels, we will use the index of the list as the labels.\n",
    "\n",
    "For brevity of time, we will only create machine learning ready data for three of the fruit collections (hence why we commented out the line for doing the entire set of fruits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME: 22.526439666748047\n"
     ]
    }
   ],
   "source": [
    "# Process all the Collections (subfolders) of Fruits\n",
    "#images = Images(labels, [l for l in range(len(labels))], config=['resize=(50,50)'], name='fruits')\n",
    "\n",
    "# For brevity, let's just do three of them\n",
    "images = Images([labels[0], labels[1], labels[2]], [l for l in range(3)], config=['resize=(50,50)'], name='fruits')\n",
    "\n",
    "print(\"TIME:\", images.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generation\n",
    "\n",
    "In the full Kaggle Fruits360 dataset, the training and test data are in separate collections. \n",
    "\n",
    "Since for this code along we are just using a subset for demonstration, we will use only images from the training set and split part of it into our test set, as well as randomize the order of the training set. We will set the split to 20% test and use 42 as our random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1309,\n",
       " 228,\n",
       " 51,\n",
       " 563,\n",
       " 501,\n",
       " 457,\n",
       " 285,\n",
       " 209,\n",
       " 1385,\n",
       " 1116,\n",
       " 178,\n",
       " 1209,\n",
       " 864,\n",
       " 65,\n",
       " 61,\n",
       " 191,\n",
       " 447,\n",
       " 476,\n",
       " 1034,\n",
       " 1232,\n",
       " 54,\n",
       " 1149,\n",
       " 407,\n",
       " 1330,\n",
       " 1436,\n",
       " 1466,\n",
       " 859,\n",
       " 451,\n",
       " 919,\n",
       " 1206,\n",
       " 569,\n",
       " 13,\n",
       " 326,\n",
       " 1429,\n",
       " 865,\n",
       " 696,\n",
       " 1445,\n",
       " 318,\n",
       " 440,\n",
       " 689,\n",
       " 1468,\n",
       " 189,\n",
       " 778,\n",
       " 198,\n",
       " 735,\n",
       " 704,\n",
       " 1236,\n",
       " 541,\n",
       " 88,\n",
       " 940,\n",
       " 1098,\n",
       " 255,\n",
       " 775,\n",
       " 161,\n",
       " 1130,\n",
       " 600,\n",
       " 1287,\n",
       " 1266,\n",
       " 740,\n",
       " 1182,\n",
       " 393,\n",
       " 142,\n",
       " 93,\n",
       " 1354,\n",
       " 466,\n",
       " 592,\n",
       " 163,\n",
       " 1458,\n",
       " 206,\n",
       " 1433,\n",
       " 1439,\n",
       " 928,\n",
       " 1301,\n",
       " 747,\n",
       " 333,\n",
       " 758,\n",
       " 727,\n",
       " 429,\n",
       " 1372,\n",
       " 546,\n",
       " 1327,\n",
       " 146,\n",
       " 1247,\n",
       " 1300,\n",
       " 350,\n",
       " 1093,\n",
       " 1471,\n",
       " 334,\n",
       " 946,\n",
       " 777,\n",
       " 552,\n",
       " 1310,\n",
       " 1140,\n",
       " 449,\n",
       " 664,\n",
       " 114,\n",
       " 469,\n",
       " 1462,\n",
       " 646,\n",
       " 821,\n",
       " 548,\n",
       " 135,\n",
       " 432,\n",
       " 1161,\n",
       " 644,\n",
       " 435,\n",
       " 1342,\n",
       " 1022,\n",
       " 810,\n",
       " 1316,\n",
       " 939,\n",
       " 292,\n",
       " 542,\n",
       " 1469,\n",
       " 505,\n",
       " 1454,\n",
       " 1103,\n",
       " 538,\n",
       " 1197,\n",
       " 877,\n",
       " 1195,\n",
       " 817,\n",
       " 741,\n",
       " 1382,\n",
       " 283,\n",
       " 1043,\n",
       " 1010,\n",
       " 186,\n",
       " 96,\n",
       " 224,\n",
       " 313,\n",
       " 1285,\n",
       " 327,\n",
       " 1463,\n",
       " 1221,\n",
       " 130,\n",
       " 788,\n",
       " 781,\n",
       " 1220,\n",
       " 958,\n",
       " 1083,\n",
       " 514,\n",
       " 1133,\n",
       " 23,\n",
       " 234,\n",
       " 1099,\n",
       " 1396,\n",
       " 1312,\n",
       " 1440,\n",
       " 1474,\n",
       " 601,\n",
       " 890,\n",
       " 323,\n",
       " 929,\n",
       " 6,\n",
       " 539,\n",
       " 1025,\n",
       " 365,\n",
       " 1039,\n",
       " 217,\n",
       " 1280,\n",
       " 611,\n",
       " 1308,\n",
       " 1317,\n",
       " 1393,\n",
       " 1453,\n",
       " 1345,\n",
       " 765,\n",
       " 330,\n",
       " 1104,\n",
       " 1086,\n",
       " 1,\n",
       " 1226,\n",
       " 663,\n",
       " 1000,\n",
       " 39,\n",
       " 229,\n",
       " 743,\n",
       " 629,\n",
       " 490,\n",
       " 118,\n",
       " 493,\n",
       " 1397,\n",
       " 1422,\n",
       " 175,\n",
       " 995,\n",
       " 141,\n",
       " 1090,\n",
       " 257,\n",
       " 262,\n",
       " 973,\n",
       " 1125,\n",
       " 338,\n",
       " 1363,\n",
       " 1080,\n",
       " 1242,\n",
       " 866,\n",
       " 433,\n",
       " 1306,\n",
       " 411,\n",
       " 638,\n",
       " 1412,\n",
       " 764,\n",
       " 897,\n",
       " 1059,\n",
       " 924,\n",
       " 247,\n",
       " 507,\n",
       " 460,\n",
       " 131,\n",
       " 692,\n",
       " 43,\n",
       " 1204,\n",
       " 1134,\n",
       " 471,\n",
       " 1205,\n",
       " 1448,\n",
       " 14,\n",
       " 145,\n",
       " 120,\n",
       " 468,\n",
       " 138,\n",
       " 64,\n",
       " 676,\n",
       " 1257,\n",
       " 1052,\n",
       " 487,\n",
       " 570,\n",
       " 994,\n",
       " 438,\n",
       " 1277,\n",
       " 270,\n",
       " 1169,\n",
       " 1180,\n",
       " 968,\n",
       " 497,\n",
       " 1241,\n",
       " 833,\n",
       " 389,\n",
       " 193,\n",
       " 1432,\n",
       " 882,\n",
       " 725,\n",
       " 867,\n",
       " 841,\n",
       " 956,\n",
       " 110,\n",
       " 201,\n",
       " 124,\n",
       " 824,\n",
       " 694,\n",
       " 223,\n",
       " 509,\n",
       " 392,\n",
       " 1237,\n",
       " 1425,\n",
       " 918,\n",
       " 287,\n",
       " 1369,\n",
       " 375,\n",
       " 1248,\n",
       " 947,\n",
       " 511,\n",
       " 154,\n",
       " 907,\n",
       " 1127,\n",
       " 200,\n",
       " 103,\n",
       " 1107,\n",
       " 30,\n",
       " 1460,\n",
       " 484,\n",
       " 340,\n",
       " 832,\n",
       " 1311,\n",
       " 985,\n",
       " 437,\n",
       " 1376,\n",
       " 1256,\n",
       " 337,\n",
       " 776,\n",
       " 4,\n",
       " 799,\n",
       " 543,\n",
       " 931,\n",
       " 584,\n",
       " 1279,\n",
       " 1138,\n",
       " 996,\n",
       " 317,\n",
       " 388,\n",
       " 607,\n",
       " 445,\n",
       " 119,\n",
       " 1110,\n",
       " 1227,\n",
       " 642,\n",
       " 117,\n",
       " 102,\n",
       " 976,\n",
       " 1029,\n",
       " 1087,\n",
       " 322,\n",
       " 116,\n",
       " 1040,\n",
       " 164,\n",
       " 380,\n",
       " 140,\n",
       " 139,\n",
       " 481,\n",
       " 826,\n",
       " 245,\n",
       " 504,\n",
       " 81,\n",
       " 167,\n",
       " 858,\n",
       " 1157,\n",
       " 1070,\n",
       " 647,\n",
       " 534,\n",
       " 418,\n",
       " 643,\n",
       " 488,\n",
       " 1192,\n",
       " 1367,\n",
       " 268,\n",
       " 614,\n",
       " 936,\n",
       " 1159,\n",
       " 148,\n",
       " 19,\n",
       " 938,\n",
       " 204,\n",
       " 150,\n",
       " 1101,\n",
       " 436,\n",
       " 1036,\n",
       " 1152,\n",
       " 271,\n",
       " 714,\n",
       " 1168,\n",
       " 500,\n",
       " 756,\n",
       " 583,\n",
       " 1323,\n",
       " 1272,\n",
       " 1112,\n",
       " 619,\n",
       " 1335,\n",
       " 16,\n",
       " 613,\n",
       " 212,\n",
       " 275,\n",
       " 1428,\n",
       " 236,\n",
       " 219,\n",
       " 1438,\n",
       " 557,\n",
       " 577,\n",
       " 431,\n",
       " 702,\n",
       " 416,\n",
       " 540,\n",
       " 1035,\n",
       " 1403,\n",
       " 1334,\n",
       " 104,\n",
       " 1434,\n",
       " 1456,\n",
       " 566,\n",
       " 90,\n",
       " 7,\n",
       " 683,\n",
       " 267,\n",
       " 536,\n",
       " 1307,\n",
       " 904,\n",
       " 875,\n",
       " 1145,\n",
       " 1299,\n",
       " 1212,\n",
       " 305,\n",
       " 73,\n",
       " 1333,\n",
       " 303,\n",
       " 880,\n",
       " 261,\n",
       " 85,\n",
       " 631,\n",
       " 746,\n",
       " 1162,\n",
       " 732,\n",
       " 430,\n",
       " 1213,\n",
       " 210,\n",
       " 724,\n",
       " 1202,\n",
       " 316,\n",
       " 1263,\n",
       " 332,\n",
       " 362,\n",
       " 844,\n",
       " 50,\n",
       " 367,\n",
       " 680,\n",
       " 843,\n",
       " 508,\n",
       " 1329,\n",
       " 1443,\n",
       " 221,\n",
       " 783,\n",
       " 79,\n",
       " 963,\n",
       " 455,\n",
       " 408,\n",
       " 942,\n",
       " 716,\n",
       " 625,\n",
       " 1411,\n",
       " 456,\n",
       " 48,\n",
       " 395,\n",
       " 816,\n",
       " 672,\n",
       " 1215,\n",
       " 1414,\n",
       " 571,\n",
       " 719,\n",
       " 1350,\n",
       " 818,\n",
       " 678,\n",
       " 56,\n",
       " 1121,\n",
       " 1156,\n",
       " 1318,\n",
       " 1188,\n",
       " 78,\n",
       " 222,\n",
       " 889,\n",
       " 707,\n",
       " 1179,\n",
       " 893,\n",
       " 1047,\n",
       " 1339,\n",
       " 1341,\n",
       " 521,\n",
       " 1105,\n",
       " 1457,\n",
       " 3,\n",
       " 403,\n",
       " 745,\n",
       " 883,\n",
       " 143,\n",
       " 971,\n",
       " 1071,\n",
       " 1060,\n",
       " 1283,\n",
       " 1275,\n",
       " 321,\n",
       " 679,\n",
       " 868,\n",
       " 127,\n",
       " 737,\n",
       " 921,\n",
       " 307,\n",
       " 519,\n",
       " 1078,\n",
       " 682,\n",
       " 1155,\n",
       " 1388,\n",
       " 412,\n",
       " 713,\n",
       " 302,\n",
       " 567,\n",
       " 1340,\n",
       " 196,\n",
       " 1210,\n",
       " 1021,\n",
       " 962,\n",
       " 1185,\n",
       " 693,\n",
       " 766,\n",
       " 1270,\n",
       " 1465,\n",
       " 630,\n",
       " 582,\n",
       " 308,\n",
       " 415,\n",
       " 561,\n",
       " 853,\n",
       " 0,\n",
       " 311,\n",
       " 293,\n",
       " 215,\n",
       " 1437,\n",
       " 804,\n",
       " 593,\n",
       " 621,\n",
       " 670,\n",
       " 329,\n",
       " 1408,\n",
       " 452,\n",
       " 974,\n",
       " 691,\n",
       " 218,\n",
       " 523,\n",
       " 1077,\n",
       " 812,\n",
       " 922,\n",
       " 815,\n",
       " 753,\n",
       " 173,\n",
       " 674,\n",
       " 86,\n",
       " 290,\n",
       " 527,\n",
       " 1016,\n",
       " 648,\n",
       " 634,\n",
       " 343,\n",
       " 95,\n",
       " 838,\n",
       " 769,\n",
       " 240,\n",
       " 688,\n",
       " 1186,\n",
       " 230,\n",
       " 825,\n",
       " 203,\n",
       " 1142,\n",
       " 25,\n",
       " 47,\n",
       " 250,\n",
       " 486,\n",
       " 1058,\n",
       " 870,\n",
       " 786,\n",
       " 74,\n",
       " 1057,\n",
       " 424,\n",
       " 1211,\n",
       " 1371,\n",
       " 589,\n",
       " 199,\n",
       " 1431,\n",
       " 1004,\n",
       " 1415,\n",
       " 506,\n",
       " 409,\n",
       " 249,\n",
       " 151,\n",
       " 671,\n",
       " 1430,\n",
       " 5,\n",
       " 914,\n",
       " 768,\n",
       " 881,\n",
       " 1032,\n",
       " 906,\n",
       " 109,\n",
       " 797,\n",
       " 1370,\n",
       " 1346,\n",
       " 180,\n",
       " 823,\n",
       " 712,\n",
       " 530,\n",
       " 475,\n",
       " 1473,\n",
       " 1051,\n",
       " 1424,\n",
       " 1015,\n",
       " 1315,\n",
       " 467,\n",
       " 136,\n",
       " 820,\n",
       " 908,\n",
       " 1359,\n",
       " 1171,\n",
       " 572,\n",
       " 609,\n",
       " 324,\n",
       " 773,\n",
       " 453,\n",
       " 627,\n",
       " 834,\n",
       " 736,\n",
       " 516,\n",
       " 1383,\n",
       " 850,\n",
       " 987,\n",
       " 1056,\n",
       " 162,\n",
       " 761,\n",
       " 1020,\n",
       " 1267,\n",
       " 265,\n",
       " 953,\n",
       " 253,\n",
       " 860,\n",
       " 652,\n",
       " 1351,\n",
       " 784,\n",
       " 796,\n",
       " 533,\n",
       " 496,\n",
       " 641,\n",
       " 244,\n",
       " 281,\n",
       " 450,\n",
       " 1064,\n",
       " 730,\n",
       " 1364,\n",
       " 952,\n",
       " 278,\n",
       " 1230,\n",
       " 1343,\n",
       " 553,\n",
       " 82,\n",
       " 1289,\n",
       " 1095,\n",
       " 1033,\n",
       " 1222,\n",
       " 710,\n",
       " 156,\n",
       " 723,\n",
       " 1120,\n",
       " 1378,\n",
       " 1148,\n",
       " 417,\n",
       " 1019,\n",
       " 555,\n",
       " 477,\n",
       " 425,\n",
       " 63,\n",
       " 211,\n",
       " 999,\n",
       " 398,\n",
       " 1219,\n",
       " 598,\n",
       " 910,\n",
       " 20,\n",
       " 1065,\n",
       " 933,\n",
       " 1031,\n",
       " 1153,\n",
       " 1321,\n",
       " 360,\n",
       " 1094,\n",
       " 771,\n",
       " 399,\n",
       " 1398,\n",
       " 551,\n",
       " 1313,\n",
       " 752,\n",
       " 559,\n",
       " 819,\n",
       " 617,\n",
       " 225,\n",
       " 499,\n",
       " 913,\n",
       " 279,\n",
       " 446,\n",
       " 1240,\n",
       " 29,\n",
       " 954,\n",
       " 344,\n",
       " 684,\n",
       " 695,\n",
       " 414,\n",
       " 1353,\n",
       " 169,\n",
       " 478,\n",
       " 1001,\n",
       " 637,\n",
       " 1068,\n",
       " 27,\n",
       " 1026,\n",
       " 606,\n",
       " 1117,\n",
       " 1045,\n",
       " 1193,\n",
       " 1088,\n",
       " 658,\n",
       " 1246,\n",
       " 1254,\n",
       " 472,\n",
       " 1348,\n",
       " 935,\n",
       " 266,\n",
       " 1187,\n",
       " 335,\n",
       " 216,\n",
       " 465,\n",
       " 1006,\n",
       " 345,\n",
       " 779,\n",
       " 785,\n",
       " 284,\n",
       " 770,\n",
       " 1450,\n",
       " 258,\n",
       " 83,\n",
       " 1166,\n",
       " 1097,\n",
       " 767,\n",
       " 1467,\n",
       " 53,\n",
       " 358,\n",
       " 1096,\n",
       " 665,\n",
       " 70,\n",
       " 667,\n",
       " 41,\n",
       " 31,\n",
       " 1356,\n",
       " 1143,\n",
       " 1324,\n",
       " 636,\n",
       " 851,\n",
       " 1452,\n",
       " 129,\n",
       " 969,\n",
       " 685,\n",
       " 1178,\n",
       " 872,\n",
       " 1224,\n",
       " 975,\n",
       " 1271,\n",
       " 1286,\n",
       " 377,\n",
       " 171,\n",
       " 620,\n",
       " 978,\n",
       " 1191,\n",
       " 751,\n",
       " 1293,\n",
       " 1119,\n",
       " 1229,\n",
       " 1410,\n",
       " 26,\n",
       " 319,\n",
       " 1079,\n",
       " 1290,\n",
       " 384,\n",
       " 406,\n",
       " 1214,\n",
       " 77,\n",
       " 915,\n",
       " 1139,\n",
       " 1129,\n",
       " 248,\n",
       " 1109,\n",
       " 930,\n",
       " 989,\n",
       " 700,\n",
       " 1360,\n",
       " 123,\n",
       " 579,\n",
       " 42,\n",
       " 355,\n",
       " 545,\n",
       " 791,\n",
       " 677,\n",
       " 379,\n",
       " 1273,\n",
       " 518,\n",
       " 1302,\n",
       " 349,\n",
       " 12,\n",
       " 839,\n",
       " 1389,\n",
       " 108,\n",
       " 443,\n",
       " 370,\n",
       " 650,\n",
       " 470,\n",
       " 1092,\n",
       " 1183,\n",
       " 912,\n",
       " 1042,\n",
       " 666,\n",
       " 276,\n",
       " 991,\n",
       " 911,\n",
       " 495,\n",
       " 748,\n",
       " 813,\n",
       " 1399,\n",
       " 274,\n",
       " 1100,\n",
       " 251,\n",
       " 1427,\n",
       " 1362,\n",
       " 461,\n",
       " 926,\n",
       " 899,\n",
       " 1132,\n",
       " 624,\n",
       " 1406,\n",
       " 809,\n",
       " 811,\n",
       " 1404,\n",
       " 1076,\n",
       " 837,\n",
       " 1053,\n",
       " 1366,\n",
       " 363,\n",
       " 264,\n",
       " 348,\n",
       " 286,\n",
       " 610,\n",
       " 282,\n",
       " 1405,\n",
       " 10,\n",
       " 529,\n",
       " 195,\n",
       " 87,\n",
       " 1269,\n",
       " 1114,\n",
       " 1262,\n",
       " 568,\n",
       " 246,\n",
       " 1249,\n",
       " 502,\n",
       " 458,\n",
       " 17,\n",
       " 955,\n",
       " 301,\n",
       " 226,\n",
       " 806,\n",
       " 998,\n",
       " 1475,\n",
       " 595,\n",
       " 1319,\n",
       " 965,\n",
       " 1106,\n",
       " 1421,\n",
       " 352,\n",
       " 1012,\n",
       " 1472,\n",
       " 845,\n",
       " 828,\n",
       " 464,\n",
       " 277,\n",
       " 1154,\n",
       " 1419,\n",
       " 854,\n",
       " 718,\n",
       " 197,\n",
       " 1131,\n",
       " 122,\n",
       " 1375,\n",
       " 1108,\n",
       " 1111,\n",
       " 1066,\n",
       " 690,\n",
       " 874,\n",
       " 603,\n",
       " 537,\n",
       " 1314,\n",
       " 289,\n",
       " 1177,\n",
       " 1235,\n",
       " 1091,\n",
       " 232,\n",
       " 369,\n",
       " 183,\n",
       " 309,\n",
       " 1258,\n",
       " 800,\n",
       " 754,\n",
       " 280,\n",
       " 46,\n",
       " 55,\n",
       " 639,\n",
       " 299,\n",
       " 604,\n",
       " 651,\n",
       " 105,\n",
       " 706,\n",
       " 587,\n",
       " 291,\n",
       " 480,\n",
       " 1296,\n",
       " 1259,\n",
       " 1225,\n",
       " 188,\n",
       " 52,\n",
       " 774,\n",
       " 489,\n",
       " 1172,\n",
       " 66,\n",
       " 410,\n",
       " 503,\n",
       " 75,\n",
       " 1455,\n",
       " 155,\n",
       " 152,\n",
       " 576,\n",
       " 984,\n",
       " 792,\n",
       " 254,\n",
       " 121,\n",
       " 1049,\n",
       " 426,\n",
       " 231,\n",
       " 535,\n",
       " 831,\n",
       " 681,\n",
       " 892,\n",
       " 304,\n",
       " 439,\n",
       " 312,\n",
       " 1461,\n",
       " 101,\n",
       " 1124,\n",
       " 1238,\n",
       " 1244,\n",
       " 1028,\n",
       " 160,\n",
       " 1164,\n",
       " 177,\n",
       " 76,\n",
       " 1074,\n",
       " 2,\n",
       " 1007,\n",
       " 1420,\n",
       " 772,\n",
       " 298,\n",
       " 33,\n",
       " 237,\n",
       " 295,\n",
       " 1048,\n",
       " 632,\n",
       " 72,\n",
       " 239,\n",
       " 1118,\n",
       " 202,\n",
       " 757,\n",
       " 1417,\n",
       " 949,\n",
       " 750,\n",
       " 272,\n",
       " 1251,\n",
       " 1390,\n",
       " 885,\n",
       " 980,\n",
       " 314,\n",
       " 1018,\n",
       " 1352,\n",
       " 1014,\n",
       " 479,\n",
       " 575,\n",
       " 1005,\n",
       " 861,\n",
       " 512,\n",
       " 1361,\n",
       " 448,\n",
       " 857,\n",
       " 40,\n",
       " 442,\n",
       " 1198,\n",
       " 256,\n",
       " 805,\n",
       " 602,\n",
       " 654,\n",
       " 492,\n",
       " 780,\n",
       " 428,\n",
       " 981,\n",
       " 294,\n",
       " 300,\n",
       " 1447,\n",
       " 661,\n",
       " 391,\n",
       " 675,\n",
       " 420,\n",
       " 137,\n",
       " 526,\n",
       " 1400,\n",
       " 390,\n",
       " 387,\n",
       " 89,\n",
       " 862,\n",
       " 879,\n",
       " 1401,\n",
       " 1377,\n",
       " 1379,\n",
       " 1050,\n",
       " 92,\n",
       " 599,\n",
       " 749,\n",
       " 325,\n",
       " 656,\n",
       " 808,\n",
       " 510,\n",
       " 491,\n",
       " 992,\n",
       " 1435,\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep all the images for training, randomize their order\n",
    "images.split = 0.2, 42\n",
    "\n",
    "# Let's look at the (internal) _train property and verify that the indices of the images has been randomized.\n",
    "images._train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now split the data. Note how the method looks similar to sci-learn's train_test_split() function, but much simpler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When used as a getter, the split property will return the training / test data and labels the same as the sci-learn\n",
    "# procedure train_test_split()\n",
    "X_train, X_test, Y_train, Y_test = images.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images 1180\n",
      "Image Example [[[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]\n",
      "\n",
      " [[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]\n",
      "\n",
      " [[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [0.99215686 1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]\n",
      "\n",
      " [[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]\n",
      "\n",
      " [[1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]\n",
      "  [1.         1.         1.        ]]]\n",
      "Label 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Images\", len(X_train))\n",
    "print(\"Image Example\", X_train[0])\n",
    "print(\"Label\", Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Tensorflow model, we need to convert our labels into a one hot encoding (dummy variable conversion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1180, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def convert_labels_to_one_hot_encoding(Y, C):\n",
    "    \"\"\" This function will do the reshape and conversion (from Coursera)\"\"\"\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "# Let's do the conversion\n",
    "Y_train = convert_labels_to_one_hot_encoding(Y_train, 3)\n",
    "Y_test  = convert_labels_to_one_hot_encoding(Y_test, 3)\n",
    "\n",
    "# Let's print the shape of our labels\n",
    "print( Y_train.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our mini-barch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.minibatch = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Construct the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vector and Output Vector and Hyperparameter Placeholders\n",
    "\n",
    "For our first tensorflow step, we will setup a Tensorflow placeholders.\n",
    "\n",
    "We have four placeholders we need to declare, one for the input vector (pixel image data, one for the output vector (fruit classifier), one for the dropout rate and one for the learning rate.\n",
    "\n",
    "For our input placeholder (which we call X), we have 7500 features (pixels per image). For the output vector (which we call Y), we have have 3 classifiers (3 different fruits). In both cases, we set the second dimension of our vector to None. The None is a placeholder for the number of samples we will feed into the neural network at run-time. We also know that our data is floating point values between 0 and 1, so we will set the data type to float32.\n",
    "\n",
    "We will declare two more placeholders for setting some hyper-parameters, the percent to keep in the dropout layer (D) and the learning rate in the optimizer (L). Since both are scalar values, we will define their shape as a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first reset our graph, so our neural network components are all declared within the same graph\n",
    "ops.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 50, 50, 3]) # shape = [batch, width, height, channels ]\n",
    "Y = tf.placeholder(tf.float32, [None, 3])  # shape = [batch, number of labels ]\n",
    "D = tf.placeholder(tf.float32, [])\n",
    "L = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT (CONVOLUTION) LAYER\n",
    "\n",
    "Let's now design our input convolution layer. For our convolutional layer, we will need a set of filters, weights for the filters and biases for the output. We will use 32 filters. Each filter will be 5 x 5 (pixels) in size and one channel (i.e., single plane) corresonding to grayscale image.\n",
    "\n",
    "Each input filter will need a weight (which our model will learn during training). The weight is multipled against the value of the input (filter), which we symbolically represent as Wx. \n",
    "\n",
    "Each output from the layer will need a bias (which our model will learn during training). The bias is added to the result of the weight multipled by the filter (Wx + b).\n",
    "\n",
    "Let's create two Tensorflow variables for our weights and biases. The weights (which we call W) will need to be a 4D matrix. The first two dimensions are the filter size (5 x 5), then the number of channels, and then the number of outputs, which will be 32.\n",
    "\n",
    "The bias will be a vector of size 32 (one for each output).\n",
    "\n",
    "We need to initialize our weights and biases to some initial value. We will initialize the weights using a random value initializer (normalized distribution) and initialize the biases to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)   # Set the same seed to get the same initialization as in this demo.\n",
    "\n",
    "# The weights for the input (convolutional) layer\n",
    "# 5x5 pixel filter, 3 channels, 32 outputs (filters)\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5 , 3, 32], stddev=0.1))\n",
    "\n",
    "# The bias for the output from the input (convolutional) layer\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together into an input (convolutional) layer. We will use the Tensorflow method tf.nn.conv2d() to apply the filters and the weights (our variable W1) against the inputs (our placeholder X), add in the bias (b1), and pass the output through a linear rectifier (RELU) activation function.\n",
    "\n",
    "- We need to reshape our flattened input data (X - which is our input placeholder) back into a 50x50 2D matric (bitmap) with three color channels - tf.reshape(X, [-1, 50, 50, 3])\n",
    "- We will set our stride for the sliding filters to move one pixel at a time in each direction.\n",
    "- We will set the padding when the filter moves past the edge of the bitmap to same.\n",
    "- Add the bias to the 32 outputs from our convolution.\n",
    "- Pass the outputs from the input (convolutional) layer through a RELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer (2D Convolution)\n",
    "\n",
    "Z1 = tf.nn.conv2d( input=X,     # tf.reshape(X, [-1, 50, 50, 3]),  \n",
    "                   filter=W1,           \n",
    "                   strides=[1,1,1,1],\n",
    "                   padding='SAME') + b1\n",
    "\n",
    "A1 = tf.nn.relu(Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the what the shape of the output tensor will be from the activation unit. \n",
    "# As you can see, it will be 50x50 pixels with 32 channels.\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX POOLING LAYER\n",
    "\n",
    "The max pooling layer will have as input the output from the first layer, which is a 4D matrix (batch, height, width, channels), where the number of channels is 32. We will use a 2x2 pooling window over each channel, with a stride of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second layer (max pooling)\n",
    "\n",
    "Z2 = tf.nn.max_pool(A1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the shape of the output tensor will be from the max pooling layer.\n",
    "# As you can see, it has been downsampled to 25x25 pixels with 32 channels.\n",
    "print(Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST HIDDEN LAYER\n",
    "\n",
    "The first hidden layer will have as inputs the flatten outputs from max pooling layer and 256 outputs. \n",
    "\n",
    "Let's start by flattening the output from the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = tf.reshape(Z2, [-1, 25*25*32])  # Flatten each 25x25 pixel with 32 channels to single 1D vector\n",
    "print(F2)\n",
    "print(F2.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input will need a weight and each output a bias (which we will train). Each output will be passed through the linear rectifier unit (RELU) activation function.\n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The return value from F2.get_shape() needs to be casted into an int.\n",
    "W3 = tf.Variable(tf.truncated_normal([int(F2.get_shape()[1]), 256], stddev=0.1))\n",
    "\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the first hidden layer\n",
    "\n",
    "- Create a node that will multiply the weights (W3) against the outputs of the max pooling layer (F2)\n",
    "- Create a node that adds the bias (b3) to the above node (F2 * W3).\n",
    "- Pass the output of the hidden layer through a dropout layer\n",
    "- Pass the outputs from the dropout layer through a RELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third layer (first hidden layer)\n",
    "Z3 = tf.add(tf.matmul(F2, W3), b3)\n",
    "\n",
    "# Let's add the dropout layer to the output signal from the second layer\n",
    "D3 = tf.nn.dropout(Z3, keep_prob=D)\n",
    "\n",
    "# Let's add the activation function to the output signal from the dropout layer\n",
    "A3 = tf.nn.relu(D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND HIDDEN LAYER\n",
    "\n",
    "The second hidden layer will have 256 inputs (outputs from first hidden layer) and 20 outputs. Each input will need a weight and each output a bias (which we will train). Each output will be passed through the linear rectifier unit (RELU) activation function.\n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W4 = tf.get_variable(\"W4\", [256, 20], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b4 = tf.get_variable(\"b4\", [1, 20], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's construct the second hidden layer\n",
    "\n",
    "- Create a node that will multiply the weights (W4) against the outputs of the first hidden layer (A3).\n",
    "- Create a node that adds the bias (b4) to the above node (W4 * A3)\n",
    "- Pass the outputs from the second hidden layer through a RELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fourth layer (second hidden layer)\n",
    "Z4 = tf.add(tf.matmul(A3, W4), b4) \n",
    "\n",
    "# Let's add the activation function to the output signal from the third layer\n",
    "A4 = tf.nn.relu(Z4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT LAYER\n",
    "\n",
    "The output layer will have 20 inputs (outputs from the second hidden layer) and 3 outputs (one for each type of fruit). Each input will need a weight and each output a bias (which we will train). The 3 outputs will be passed through a softmax activation function. \n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W5 = tf.get_variable(\"W5\", [20, 3], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b5 = tf.get_variable(\"b5\", [1, 3], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the output layer\n",
    "\n",
    "- Create a node that will multiply the weights (W4) against the outputs of the second hidden layer (A3).\n",
    "- Create a node that adds the bias to the above node (W4 * A3).\n",
    "- Pass the outputs from the output layer through a SOFTMAX squashing function (done by the optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fifth layer (output layer)\n",
    "Z5 = tf.add(tf.matmul(A4, W5), b5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMIZER\n",
    "\n",
    "Now its time to design our optimizer. Let's start by designing our cost function. We will use the mean value of the softmax cross entropy between the predicted labels and actual labels. This is what we want to reduce on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z5, labels=Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design our optimizer. This is the method that adjusts the values of the weights and biases, based on minimizing the cost value during training.\n",
    "\n",
    "We also need to set a learning rate. This is multiplied against the gradient calculation. It's used to prevent huge swings in setting weights which can result in either converging at a local (instead of global) optima, or not converging at all (infinite gradient). We will set the learning rate when we run the graph using the placeholder L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate for Gradient Descent algorithm\n",
    "# learning_rate = 0.5\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(L).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph\n",
    "\n",
    "We've built our Tensorflow graph for training our data. So, let's start training it.\n",
    "\n",
    "First, we need to call Tensorflow's global_variables_initializer() method to initialize the variables we've defined. We will create this as another node, which will be the first node we run (evaluate) in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also a good idea to know how long your training takes, so let's import the time library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our hyperparameters.\n",
    "\n",
    "We need to set the number of epochs (that's how many times we run the training data through the neural network), and the batch size. The batch size is a small subset of the entire training set. We will be running a batch at a time per epoch. After each batch, then the cost is computed and backpropagated through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 25                                    # run a 25 epochs\n",
    "batch_size = 32                                # for each epoch, train in batches of 100 images\n",
    "number_of_images = len(X_train)                # number of images in training data\n",
    "\n",
    "# Feed Dictionary Parameters\n",
    "keep_prob = 0.9                                # percent of outputs to keep in dropout layer\n",
    "learning_rate = 0.02                           # the learning rate for graident descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start = time.time()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # number of batches in an epoch\n",
    "        batches = number_of_images // batch_size\n",
    "\n",
    "        # run our training data through the neural network for each epoch\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "          epoch_cost = 0\n",
    "\n",
    "          # Run the training data through the neural network\n",
    "          for batch in range(batches):\n",
    "\n",
    "              # Calculate the start and end indices for the next batch\n",
    "              begin = (batch * batch_size)\n",
    "              end   = (batch * batch_size) + batch_size\n",
    "\n",
    "\n",
    "              # Get the next sequential batch from the training data\n",
    "              batch_xs, batch_ys = X_train[begin:end], Y_train[begin:end]\n",
    "\n",
    "              # Feed this batch through the neural network.\n",
    "              _, batch_cost = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys, D: keep_prob, L: learning_rate})\n",
    "\n",
    "              epoch_cost += batch_cost\n",
    "\n",
    "          print(\"Epoch: \", epoch, epoch_cost / batches)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(\"Training Time:\", end - start)\n",
    "\n",
    "        # Test the Model\n",
    "\n",
    "        # Let's select the highest percent from the softmax output per image as the prediction.\n",
    "        prediction = tf.equal(tf.argmax(Z5), tf.argmax(Y))\n",
    "\n",
    "        # Let's create another node for calculating the accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "        # Now let's run our trainingt images through the model to calculate our accuracy during training\n",
    "        # Note how we set the keep percent for the dropout rate to 1.0 (no dropout) when we are evaluating the accuracy.\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train, D: 1.0}))\n",
    "\n",
    "        # Now let's run our test images through the model to calculate our accuracy on the test data\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test, D: 1.0}))\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
